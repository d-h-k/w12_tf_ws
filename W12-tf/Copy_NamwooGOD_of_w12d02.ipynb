{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of w12d02",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-h-k/w12_tf_ws/blob/master/Copy_NamwooGOD_of_w12d02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmosFJc4dqOo",
        "colab_type": "text"
      },
      "source": [
        "# Cost and Gradient 57p"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrpblYcGcy17",
        "colab_type": "code",
        "outputId": "0b7ecd85-f44e-4969-baee-d74f0606caa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = [1., 2., 3.]\n",
        "Y = [1., 2., 3.]\n",
        "m = len(X)\n",
        "6\n",
        "W = tf.placeholder(tf.float32)\n",
        "\n",
        "hypothesis = tf.multiply(W, X)\n",
        "# cf. ) tf.mul(W, X) using in the old version\n",
        "cost = tf.reduce_sum(tf.pow(hypothesis-Y, 2)) / m\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "#cf.)tf.initialize_all_variables()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "W_val, cost_val = [], []\n",
        "\n",
        "for i in range(-30, 51) :\n",
        "  xPos = i * 0.1\n",
        "  yPos = sess.run(cost ,feed_dict={W: xPos})\n",
        "#   print('{:3.1f}, {:3.1f}'.format(xPos, yPos))  # : 앞 형식문자. \n",
        "\n",
        "  W_val.append(xPos)\n",
        "  cost_val.append(yPos)\n",
        "sess. close()\n",
        "\n",
        "# plt.xlim()\n",
        "plt.plot(W_val, cost_val, 'ro')\n",
        "plt.ylabel('Cost')\n",
        "plt.xlabel('W')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmdXc/wf9Q+6lHZ3+FQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0ldehaVcyuw",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression 59p"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O1B_SDGcSXw",
        "colab_type": "code",
        "outputId": "05313847-e260-4bcf-868d-c75561d717d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x_data = [1., 2., 3.]\n",
        "y_data = [1., 2., 3.]\n",
        "\n",
        "# try to find values for wand b that compute y_data W * x_data + b\n",
        "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
        "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
        "\n",
        "# my hypothesis\n",
        "hypothesis = W * x_data + b\n",
        "\n",
        "# Simplified cost function\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "# minimize\n",
        "rate = tf.Variable(0.1) # learning rate, alpha\n",
        "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "# before starting, initialize the variables. We will 'run' this first.\n",
        "init = tf.global_variables_initializer()\n",
        "#cf.)tf.initialize_all_variables()\n",
        "\n",
        "# launch the graph\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "# fit the line\n",
        "for step in range(2001):\n",
        "  sess.run( train )\n",
        "  if step% 20 == 0:\n",
        "    print('{:4} {} {} {}'.format (step, sess.run(cost), sess.run(W), sess.run(b)))\n",
        "\n",
        "    # learns best fit is W: [1] b: [0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0 0.1918504685163498 [0.79173803] [0.82017606]\n",
            "  20 0.031633567065000534 [0.79342866] [0.46958515]\n",
            "  40 0.01195200253278017 [0.8730255] [0.28864282]\n",
            "  60 0.004515789914876223 [0.92195183] [0.17742191]\n",
            "  80 0.0017061849357560277 [0.95202553] [0.10905708]\n",
            " 100 0.0006446472834795713 [0.9705113] [0.06703483]\n",
            " 120 0.00024356378708034754 [0.981874] [0.04120471]\n",
            " 140 9.202522778650746e-05 [0.9888584] [0.02532757]\n",
            " 160 3.476988786133006e-05 [0.9931515] [0.01556828]\n",
            " 180 1.3137199857737869e-05 [0.9957904] [0.00956949]\n",
            " 200 4.963432274962543e-06 [0.99741244] [0.00588215]\n",
            " 220 1.875302928056044e-06 [0.99840945] [0.00361561]\n",
            " 240 7.08527466031228e-07 [0.9990223] [0.00222247]\n",
            " 260 2.676803774193104e-07 [0.999399] [0.00136611]\n",
            " 280 1.0113507187270443e-07 [0.9996306] [0.00083969]\n",
            " 300 3.823677374725776e-08 [0.9997729] [0.00051614]\n",
            " 320 1.4438138684624846e-08 [0.9998604] [0.00031728]\n",
            " 340 5.457195584313013e-09 [0.99991417] [0.000195]\n",
            " 360 2.0570354308802052e-09 [0.9999473] [0.00011986]\n",
            " 380 7.787359090904999e-10 [0.9999676] [7.365451e-05]\n",
            " 400 2.9257307687657885e-10 [0.99998003] [4.5242963e-05]\n",
            " 420 1.0970779840135947e-10 [0.9999877] [2.7830458e-05]\n",
            " 440 4.178464865778331e-11 [0.9999925] [1.7109567e-05]\n",
            " 460 1.5560885913146194e-11 [0.99999535] [1.0497423e-05]\n",
            " 480 5.9685589803848416e-12 [0.99999714] [6.4443093e-06]\n",
            " 500 2.0842587638431054e-12 [0.9999982] [3.9727033e-06]\n",
            " 520 1.0042336970675358e-12 [0.9999989] [2.4547721e-06]\n",
            " 540 3.268496584496461e-13 [0.99999934] [1.5169926e-06]\n",
            " 560 9.47390291759255e-14 [0.9999996] [9.05052e-07]\n",
            " 580 6.158037269129654e-14 [0.99999964] [6.904755e-07]\n",
            " 600 6.158037269129654e-14 [0.99999976] [5.6331896e-07]\n",
            " 620 3.789561370324927e-14 [0.9999999] [3.0900574e-07]\n",
            " 640 4.736951712906159e-15 [0.99999994] [1.3416536e-07]\n",
            " 660 0.0 [1.] [5.46925e-08]\n",
            " 680 0.0 [1.] [5.46925e-08]\n",
            " 700 0.0 [1.] [5.46925e-08]\n",
            " 720 0.0 [1.] [5.46925e-08]\n",
            " 740 0.0 [1.] [5.46925e-08]\n",
            " 760 0.0 [1.] [5.46925e-08]\n",
            " 780 0.0 [1.] [5.46925e-08]\n",
            " 800 0.0 [1.] [5.46925e-08]\n",
            " 820 0.0 [1.] [5.46925e-08]\n",
            " 840 0.0 [1.] [5.46925e-08]\n",
            " 860 0.0 [1.] [5.46925e-08]\n",
            " 880 0.0 [1.] [5.46925e-08]\n",
            " 900 0.0 [1.] [5.46925e-08]\n",
            " 920 0.0 [1.] [5.46925e-08]\n",
            " 940 0.0 [1.] [5.46925e-08]\n",
            " 960 0.0 [1.] [5.46925e-08]\n",
            " 980 0.0 [1.] [5.46925e-08]\n",
            "1000 0.0 [1.] [5.46925e-08]\n",
            "1020 0.0 [1.] [5.46925e-08]\n",
            "1040 0.0 [1.] [5.46925e-08]\n",
            "1060 0.0 [1.] [5.46925e-08]\n",
            "1080 0.0 [1.] [5.46925e-08]\n",
            "1100 0.0 [1.] [5.46925e-08]\n",
            "1120 0.0 [1.] [5.46925e-08]\n",
            "1140 0.0 [1.] [5.46925e-08]\n",
            "1160 0.0 [1.] [5.46925e-08]\n",
            "1180 0.0 [1.] [5.46925e-08]\n",
            "1200 0.0 [1.] [5.46925e-08]\n",
            "1220 0.0 [1.] [5.46925e-08]\n",
            "1240 0.0 [1.] [5.46925e-08]\n",
            "1260 0.0 [1.] [5.46925e-08]\n",
            "1280 0.0 [1.] [5.46925e-08]\n",
            "1300 0.0 [1.] [5.46925e-08]\n",
            "1320 0.0 [1.] [5.46925e-08]\n",
            "1340 0.0 [1.] [5.46925e-08]\n",
            "1360 0.0 [1.] [5.46925e-08]\n",
            "1380 0.0 [1.] [5.46925e-08]\n",
            "1400 0.0 [1.] [5.46925e-08]\n",
            "1420 0.0 [1.] [5.46925e-08]\n",
            "1440 0.0 [1.] [5.46925e-08]\n",
            "1460 0.0 [1.] [5.46925e-08]\n",
            "1480 0.0 [1.] [5.46925e-08]\n",
            "1500 0.0 [1.] [5.46925e-08]\n",
            "1520 0.0 [1.] [5.46925e-08]\n",
            "1540 0.0 [1.] [5.46925e-08]\n",
            "1560 0.0 [1.] [5.46925e-08]\n",
            "1580 0.0 [1.] [5.46925e-08]\n",
            "1600 0.0 [1.] [5.46925e-08]\n",
            "1620 0.0 [1.] [5.46925e-08]\n",
            "1640 0.0 [1.] [5.46925e-08]\n",
            "1660 0.0 [1.] [5.46925e-08]\n",
            "1680 0.0 [1.] [5.46925e-08]\n",
            "1700 0.0 [1.] [5.46925e-08]\n",
            "1720 0.0 [1.] [5.46925e-08]\n",
            "1740 0.0 [1.] [5.46925e-08]\n",
            "1760 0.0 [1.] [5.46925e-08]\n",
            "1780 0.0 [1.] [5.46925e-08]\n",
            "1800 0.0 [1.] [5.46925e-08]\n",
            "1820 0.0 [1.] [5.46925e-08]\n",
            "1840 0.0 [1.] [5.46925e-08]\n",
            "1860 0.0 [1.] [5.46925e-08]\n",
            "1880 0.0 [1.] [5.46925e-08]\n",
            "1900 0.0 [1.] [5.46925e-08]\n",
            "1920 0.0 [1.] [5.46925e-08]\n",
            "1940 0.0 [1.] [5.46925e-08]\n",
            "1960 0.0 [1.] [5.46925e-08]\n",
            "1980 0.0 [1.] [5.46925e-08]\n",
            "2000 0.0 [1.] [5.46925e-08]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk30lnyifSP5",
        "colab_type": "text"
      },
      "source": [
        "# Test of Linear Regression 60p"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76pVZFYAa3wO",
        "colab_type": "code",
        "outputId": "7b1d8ff8-0b2e-4ce5-8069-aeea78b37b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x_data = [1., 2., 3., 4.]\n",
        "y_data = [2., 4., 6., 8.]\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "\n",
        "with tf.name_scope(\"Logit_Layer\"):\n",
        "# range is -100 ~ 100\n",
        "  W = tf.Variable(tf.random_uniform ([1], -100., 100.))\n",
        "  b = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
        "  hypothesis = W * X + b\n",
        "  \n",
        "with tf.name_scope(\"GD_Trainer\"):\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "  rate = tf.Variable(0.1)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(rate)\n",
        "  train = optimizer.minimize(cost)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "#cf.)tf.initialize_all_variables() using in the old version\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "writer = tf.summary.FileWriter('/content/log',graph=tf.get_default_graph())\n",
        "\n",
        "\n",
        "#여기서부터 train\n",
        "for step in range(700):\n",
        "  sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
        "  if step% 5 == 0:\n",
        "    print('step:',step ,sess.run(cost, feed_dict={X: x_data, Y: y_data}), 'W:',sess.run(W), 'b:',sess.run(b))\n",
        "\n",
        "#여기서부터 test\n",
        "#관측되지 않은 데이터 넣어보기 5, 2.5\n",
        "print(sess.run(hypothesis, feed_dict={X : 5}))    \n",
        "print(sess.run(hypothesis, feed_dict={X : 2.5}))\n",
        "print(sess.run(hypothesis, feed_dict={X: [2.5, 5]}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 0 5838.243 W: [-38.441177] b: [39.508446]\n",
            "step: 5 380.44974 W: [-8.678916] b: [42.12132]\n",
            "step: 10 209.07169 W: [-10.423141] b: [35.077015]\n",
            "step: 15 152.96408 W: [-8.232549] b: [30.280603]\n",
            "step: 20 112.84649 W: [-6.849095] b: [25.99099]\n",
            "step: 25 83.26761 W: [-5.5933986] b: [22.329088]\n",
            "step: 30 61.44214 W: [-4.5238414] b: [19.180407]\n",
            "step: 35 45.337395 W: [-3.6038587] b: [16.476093]\n",
            "step: 40 33.45391 W: [-2.8137567] b: [14.153023]\n",
            "step: 45 24.685234 W: [-2.1350336] b: [12.157503]\n",
            "step: 50 18.214931 W: [-1.5520099] b: [10.443342]\n",
            "step: 55 13.440577 W: [-1.0511905] b: [8.970873]\n",
            "step: 60 9.917636 W: [-0.6209849] b: [7.706015]\n",
            "step: 65 7.318102 W: [-0.25143626] b: [6.619497]\n",
            "step: 70 5.399938 W: [0.0660074] b: [5.686174]\n",
            "step: 75 3.9845486 W: [0.33869267] b: [4.884446]\n",
            "step: 80 2.9401498 W: [0.5729306] b: [4.1957583]\n",
            "step: 85 2.1694999 W: [0.77414167] b: [3.604173]\n",
            "step: 90 1.6008477 W: [0.94698286] b: [3.0959988]\n",
            "step: 95 1.1812459 W: [1.0954542] b: [2.659475]\n",
            "step: 100 0.871627 W: [1.2229915] b: [2.2844996]\n",
            "step: 105 0.6431629 W: [1.3325467] b: [1.9623942]\n",
            "step: 110 0.4745821 W: [1.426655] b: [1.6857044]\n",
            "step: 115 0.3501881 W: [1.5074946] b: [1.4480267]\n",
            "step: 120 0.25839958 W: [1.5769358] b: [1.2438605]\n",
            "step: 125 0.19066986 W: [1.6365862] b: [1.068481]\n",
            "step: 130 0.14069295 W: [1.687826] b: [0.9178293]\n",
            "step: 135 0.103815585 W: [1.7318416] b: [0.78841907]\n",
            "step: 140 0.07660425 W: [1.7696506] b: [0.677255]\n",
            "step: 145 0.05652534 W: [1.802129] b: [0.5817647]\n",
            "step: 150 0.041709322 W: [1.8300282] b: [0.49973816]\n",
            "step: 155 0.03077684 W: [1.8539935] b: [0.42927706]\n",
            "step: 160 0.022709828 W: [1.8745799] b: [0.3687507]\n",
            "step: 165 0.016757324 W: [1.8922637] b: [0.31675833]\n",
            "step: 170 0.012365019 W: [1.907454] b: [0.2720966]\n",
            "step: 175 0.00912399 W: [1.9205025] b: [0.23373201]\n",
            "step: 180 0.0067324867 W: [1.9317116] b: [0.20077676]\n",
            "step: 185 0.0049678143 W: [1.9413397] b: [0.17246799]\n",
            "step: 190 0.003665693 W: [1.9496106] b: [0.14815064]\n",
            "step: 195 0.002704861 W: [1.9567155] b: [0.12726207]\n",
            "step: 200 0.0019958937 W: [1.9628184] b: [0.10931862]\n",
            "step: 205 0.001472746 W: [1.9680609] b: [0.09390512]\n",
            "step: 210 0.0010867177 W: [1.9725642] b: [0.0806649]\n",
            "step: 215 0.00080187543 W: [1.9764323] b: [0.06929138]\n",
            "step: 220 0.000591696 W: [1.9797554] b: [0.05952162]\n",
            "step: 225 0.00043660076 W: [1.9826099] b: [0.05112931]\n",
            "step: 230 0.00032216418 W: [1.9850619] b: [0.04392031]\n",
            "step: 235 0.00023772051 W: [1.987168] b: [0.03772764]\n",
            "step: 240 0.00017540931 W: [1.9889773] b: [0.03240819]\n",
            "step: 245 0.00012943347 W: [1.9905314] b: [0.02783875]\n",
            "step: 250 9.5508e-05 W: [1.9918665] b: [0.02391361]\n",
            "step: 255 7.0474285e-05 W: [1.9930133] b: [0.0205419]\n",
            "step: 260 5.200241e-05 W: [1.9939983] b: [0.01764554]\n",
            "step: 265 3.837083e-05 W: [1.9948446] b: [0.0151576]\n",
            "step: 270 2.8313261e-05 W: [1.9955714] b: [0.01302037]\n",
            "step: 275 2.089197e-05 W: [1.9961958] b: [0.01118454]\n",
            "step: 280 1.5416039e-05 W: [1.9967322] b: [0.0096076]\n",
            "step: 285 1.1375131e-05 W: [1.997193] b: [0.00825297]\n",
            "step: 290 8.393945e-06 W: [1.9975888] b: [0.0070893]\n",
            "step: 295 6.19346e-06 W: [1.9979289] b: [0.00608979]\n",
            "step: 300 4.5703405e-06 W: [1.9982207] b: [0.00523107]\n",
            "step: 305 3.3721356e-06 W: [1.9984717] b: [0.00449357]\n",
            "step: 310 2.488443e-06 W: [1.998687] b: [0.00385991]\n",
            "step: 315 1.8361147e-06 W: [1.9988723] b: [0.00331569]\n",
            "step: 320 1.3547817e-06 W: [1.9990313] b: [0.00284818]\n",
            "step: 325 9.995765e-07 W: [1.9991678] b: [0.00244656]\n",
            "step: 330 7.3763965e-07 W: [1.9992853] b: [0.00210167]\n",
            "step: 335 5.441767e-07 W: [1.999386] b: [0.00180528]\n",
            "step: 340 4.0173143e-07 W: [1.9994725] b: [0.00155072]\n",
            "step: 345 2.9631906e-07 W: [1.999547] b: [0.00133209]\n",
            "step: 350 2.186418e-07 W: [1.9996108] b: [0.00114423]\n",
            "step: 355 1.6139359e-07 W: [1.9996656] b: [0.00098288]\n",
            "step: 360 1.19089464e-07 W: [1.999713] b: [0.00084438]\n",
            "step: 365 8.78555e-08 W: [1.9997532] b: [0.00072529]\n",
            "step: 370 6.481322e-08 W: [1.9997882] b: [0.00062306]\n",
            "step: 375 4.7849383e-08 W: [1.999818] b: [0.0005352]\n",
            "step: 380 3.532155e-08 W: [1.9998437] b: [0.0004598]\n",
            "step: 385 2.6067099e-08 W: [1.9998657] b: [0.00039498]\n",
            "step: 390 1.9230697e-08 W: [1.9998845] b: [0.00033926]\n",
            "step: 395 1.4197909e-08 W: [1.9999009] b: [0.0002915]\n",
            "step: 400 1.0466479e-08 W: [1.9999149] b: [0.00025036]\n",
            "step: 405 7.72458e-09 W: [1.9999268] b: [0.00021505]\n",
            "step: 410 5.7003717e-09 W: [1.9999373] b: [0.00018475]\n",
            "step: 415 4.1965365e-09 W: [1.9999461] b: [0.00015867]\n",
            "step: 420 3.0955931e-09 W: [1.9999536] b: [0.00013629]\n",
            "step: 425 2.2876634e-09 W: [1.9999602] b: [0.00011707]\n",
            "step: 430 1.6892869e-09 W: [1.9999659] b: [0.00010062]\n",
            "step: 435 1.2436345e-09 W: [1.9999707] b: [8.6366286e-05]\n",
            "step: 440 9.173391e-10 W: [1.9999748] b: [7.417118e-05]\n",
            "step: 445 6.7745987e-10 W: [1.9999783] b: [6.365692e-05]\n",
            "step: 450 4.9715254e-10 W: [1.9999815] b: [5.4716224e-05]\n",
            "step: 455 3.694396e-10 W: [1.9999839] b: [4.6931862e-05]\n",
            "step: 460 2.7091573e-10 W: [1.9999863] b: [4.035151e-05]\n",
            "step: 465 1.9998936e-10 W: [1.9999881] b: [3.4629466e-05]\n",
            "step: 470 1.4803447e-10 W: [1.99999] b: [2.982533e-05]\n",
            "step: 475 1.08641984e-10 W: [1.9999913] b: [2.5569558e-05]\n",
            "step: 480 8.0220275e-11 W: [1.9999926] b: [2.2005199e-05]\n",
            "step: 485 5.9174e-11 W: [1.9999936] b: [1.8869994e-05]\n",
            "step: 490 4.3840487e-11 W: [1.9999945] b: [1.624739e-05]\n",
            "step: 495 3.2059688e-11 W: [1.9999952] b: [1.3922809e-05]\n",
            "step: 500 2.377476e-11 W: [1.999996] b: [1.1979698e-05]\n",
            "step: 505 1.7521984e-11 W: [1.9999965] b: [1.0310767e-05]\n",
            "step: 510 1.297451e-11 W: [1.999997] b: [8.856414e-06]\n",
            "step: 515 9.109158e-12 W: [1.9999975] b: [7.5927956e-06]\n",
            "step: 520 6.934897e-12 W: [1.9999977] b: [6.5079917e-06]\n",
            "step: 525 5.456968e-12 W: [1.9999981] b: [5.625843e-06]\n",
            "step: 530 4.206413e-12 W: [1.9999985] b: [4.8152197e-06]\n",
            "step: 535 2.742695e-12 W: [1.9999986] b: [4.123806e-06]\n",
            "step: 540 2.344791e-12 W: [1.9999987] b: [3.5396806e-06]\n",
            "step: 545 1.7053026e-12 W: [1.9999989] b: [3.0866854e-06]\n",
            "step: 550 1.1510792e-12 W: [1.999999] b: [2.6575321e-06]\n",
            "step: 555 1.0373924e-12 W: [1.9999993] b: [2.287983e-06]\n",
            "step: 560 7.9580786e-13 W: [1.9999994] b: [1.9303554e-06]\n",
            "step: 565 5.684342e-13 W: [1.9999994] b: [1.6204112e-06]\n",
            "step: 570 3.410605e-13 W: [1.9999995] b: [1.4058346e-06]\n",
            "step: 575 2.4158453e-13 W: [1.9999995] b: [1.2031788e-06]\n",
            "step: 580 2.4158453e-13 W: [1.9999996] b: [1.0482067e-06]\n",
            "step: 585 1.1368684e-13 W: [1.9999996] b: [8.8131367e-07]\n",
            "step: 590 1.1368684e-13 W: [1.9999998] b: [7.740253e-07]\n",
            "step: 595 1.7053026e-13 W: [1.9999998] b: [6.54816e-07]\n",
            "step: 600 1.1368684e-13 W: [1.9999999] b: [6.3097406e-07]\n",
            "step: 605 1.2789769e-13 W: [1.9999998] b: [5.594485e-07]\n",
            "step: 610 1.1368684e-13 W: [1.9999999] b: [5.7136947e-07]\n",
            "step: 615 1.2789769e-13 W: [1.9999998] b: [4.998439e-07]\n",
            "step: 620 1.1368684e-13 W: [1.9999999] b: [5.117649e-07]\n",
            "step: 625 1.2789769e-13 W: [1.9999998] b: [4.4023932e-07]\n",
            "step: 630 1.4210855e-14 W: [1.9999999] b: [4.2831843e-07]\n",
            "step: 635 1.4210855e-14 W: [1.9999999] b: [3.6871384e-07]\n",
            "step: 640 1.4210855e-14 W: [1.9999999] b: [3.0910925e-07]\n",
            "step: 645 1.4210855e-14 W: [1.9999999] b: [2.4950467e-07]\n",
            "step: 650 1.1368684e-13 W: [1.9999999] b: [2.1374188e-07]\n",
            "step: 655 1.4210855e-14 W: [2.] b: [2.3758376e-07]\n",
            "step: 660 1.4210855e-14 W: [2.] b: [1.779791e-07]\n",
            "step: 665 0.0 W: [2.] b: [1.18374444e-07]\n",
            "step: 670 0.0 W: [2.] b: [1.18374444e-07]\n",
            "step: 675 0.0 W: [2.] b: [1.18374444e-07]\n",
            "step: 680 0.0 W: [2.] b: [1.18374444e-07]\n",
            "step: 685 0.0 W: [2.] b: [1.18374444e-07]\n",
            "step: 690 0.0 W: [2.] b: [1.18374444e-07]\n",
            "step: 695 0.0 W: [2.] b: [1.18374444e-07]\n",
            "[10.]\n",
            "[5.]\n",
            "[ 5. 10.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRfkoRdAuU2m",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Descent 응용 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-mNybJRuZZ0",
        "colab_type": "code",
        "outputId": "541c7262-a66f-4e9b-a533-20d8b9d1a487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x_data = [1., 2., 3., 4.]\n",
        "y_data = [2., 4., 6., 8.]\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "\n",
        "with tf.name_scope(\"Logit_Layer\"):\n",
        "# range is -100 ~ 100\n",
        "  W = tf.Variable(tf.random_uniform ([1], -100., 100.))\n",
        "  W2 = tf.Variable(tf.random_uniform ([1], -100., 100.))\n",
        "\n",
        "  #b = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
        "  hypothesis = W * X # + b\n",
        "  \n",
        "with tf.name_scope(\"GD_Trainer\"):\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - Y))  #cost\n",
        "  rate = tf.Variable(0.1)\n",
        "  gradient = tf.reduce_mean((W * X - Y) * X)    #gradient 실습하기 위해 추가\n",
        "  descent = W - rate * gradient   #alpha 는 Learning rate~  # descent \n",
        "  update = tf.assign(W2, descent)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(rate)\n",
        "  train = optimizer.minimize(cost)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "#cf.)tf.initialize_all_variables() using in the old version\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "writer = tf.summary.FileWriter('/content/log',graph=tf.get_default_graph())\n",
        "\n",
        "#여기서부터 train\n",
        "for step in range(200):\n",
        "  sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
        "  if step% 5 == 0:\n",
        "    print('step:',step ,sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(update, feed_dict={X: x_data, Y: y_data}),'W:',sess.run(W), 'W2:',sess.run(W2))\n",
        "\n",
        "#여기서부터 test\n",
        "#관측되지 않은 데이터 넣어보기 5, 2.5\n",
        "print(sess.run(hypothesis, feed_dict={X : 5}))    \n",
        "print(sess.run(hypothesis, feed_dict={X : 2.5}))\n",
        "print(sess.run(hypothesis, feed_dict={X: [2.5, 5]}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 0 487.69998 [4.015978] W: [10.063911] W2: [4.015978]\n",
            "step: 5 0.4762696 [1.9370008] W: [1.7480028] W2: [1.9370008]\n",
            "step: 10 0.00046511312 [2.0019689] W: [2.007875] W2: [2.0019689]\n",
            "step: 15 4.5404704e-07 [1.9999385] W: [1.999754] W2: [1.9999385]\n",
            "step: 20 4.3655746e-10 [2.000002] W: [2.0000076] W2: [2.000002]\n",
            "step: 25 5.258016e-13 [2.] W: [1.9999998] W2: [2.]\n",
            "step: 30 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 35 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 40 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 45 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 50 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 55 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 60 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 65 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 70 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 75 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 80 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 85 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 90 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 95 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 100 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 105 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 110 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 115 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 120 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 125 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 130 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 135 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 140 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 145 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 150 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 155 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 160 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 165 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 170 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 175 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 180 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 185 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 190 0.0 [2.] W: [2.] W2: [2.]\n",
            "step: 195 0.0 [2.] W: [2.] W2: [2.]\n",
            "[10.]\n",
            "[5.]\n",
            "[ 5. 10.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1WFUEzT0ioV",
        "colab_type": "text"
      },
      "source": [
        "## 이어서2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vNiIU8b0qQ-",
        "colab_type": "code",
        "outputId": "67aaef3b-cfba-461a-cc44-12df16349771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x_data = [1., 2., 3., 4.]\n",
        "y_data = [2., 4., 6., 8.]\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "# W2 = tf.placeholder(tf.float32)\n",
        "\n",
        "\n",
        "with tf.name_scope(\"Logit_Layer\"):\n",
        "# range is -100 ~ 100\n",
        "#   W = tf.Variable(tf.random_uniform ([1], -100., 100.))\n",
        "  W2 = tf.Variable(tf.random_uniform ([1], -100., 100.))\n",
        "  #b = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
        "  hypothesis = W2 * X # + b\n",
        "  \n",
        "with tf.name_scope(\"GD_Trainer\"):\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - Y))  #cost\n",
        "  rate = tf.Variable(0.1)\n",
        "  gradient = tf.reduce_mean((W2 * X - Y) * X)    #gradient 실습하기 위해 추가\n",
        "  descent = W2 - rate * gradient   #alpha 는 Learning rate~  # descent \n",
        "  update = tf.assign(W2, descent)\n",
        "#   optimizer = tf.train.GradientDescentOptimizer(rate)\n",
        "#   train = optimizer.minimize(cost)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "#cf.)tf.initialize_all_variables() using in the old version\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "writer = tf.summary.FileWriter('/content/log',graph=tf.get_default_graph())\n",
        "\n",
        "#여기서부터 train\n",
        "for step in range(200):\n",
        "#   sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
        "  if step% 5 == 0:\n",
        "    print('step:', step, sess.run(update, feed_dict={X: x_data, Y: y_data}),'W2:',sess.run(W2))\n",
        "\n",
        "#여기서부터 test\n",
        "#관측되지 않은 데이터 넣어보기 5, 2.5\n",
        "print(sess.run(hypothesis, feed_dict={X : 5}))    \n",
        "print(sess.run(hypothesis, feed_dict={X : 2.5}))\n",
        "print(sess.run(hypothesis, feed_dict={X: [2.5, 5]}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 0 [8.9156] W2: [8.9156]\n",
            "step: 5 [3.7289] W2: [3.7289]\n",
            "step: 10 [2.432225] W2: [2.432225]\n",
            "step: 15 [2.1080563] W2: [2.1080563]\n",
            "step: 20 [2.027014] W2: [2.027014]\n",
            "step: 25 [2.0067534] W2: [2.0067534]\n",
            "step: 30 [2.0016885] W2: [2.0016885]\n",
            "step: 35 [2.000422] W2: [2.000422]\n",
            "step: 40 [2.0001054] W2: [2.0001054]\n",
            "step: 45 [2.0000262] W2: [2.0000262]\n",
            "step: 50 [2.0000067] W2: [2.0000067]\n",
            "step: 55 [2.0000017] W2: [2.0000017]\n",
            "step: 60 [2.0000005] W2: [2.0000005]\n",
            "step: 65 [2.] W2: [2.]\n",
            "step: 70 [2.] W2: [2.]\n",
            "step: 75 [2.] W2: [2.]\n",
            "step: 80 [2.] W2: [2.]\n",
            "step: 85 [2.] W2: [2.]\n",
            "step: 90 [2.] W2: [2.]\n",
            "step: 95 [2.] W2: [2.]\n",
            "step: 100 [2.] W2: [2.]\n",
            "step: 105 [2.] W2: [2.]\n",
            "step: 110 [2.] W2: [2.]\n",
            "step: 115 [2.] W2: [2.]\n",
            "step: 120 [2.] W2: [2.]\n",
            "step: 125 [2.] W2: [2.]\n",
            "step: 130 [2.] W2: [2.]\n",
            "step: 135 [2.] W2: [2.]\n",
            "step: 140 [2.] W2: [2.]\n",
            "step: 145 [2.] W2: [2.]\n",
            "step: 150 [2.] W2: [2.]\n",
            "step: 155 [2.] W2: [2.]\n",
            "step: 160 [2.] W2: [2.]\n",
            "step: 165 [2.] W2: [2.]\n",
            "step: 170 [2.] W2: [2.]\n",
            "step: 175 [2.] W2: [2.]\n",
            "step: 180 [2.] W2: [2.]\n",
            "step: 185 [2.] W2: [2.]\n",
            "step: 190 [2.] W2: [2.]\n",
            "step: 195 [2.] W2: [2.]\n",
            "[10.]\n",
            "[5.]\n",
            "[ 5. 10.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNGTnhzrcQYN",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression 62p"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DCXK4pKcGMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}